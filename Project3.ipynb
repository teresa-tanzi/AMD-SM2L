{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint project AMD + SM2L\n",
    "\n",
    "https://docs.google.com/document/d/1oqoIyRUI_digfIokf53fox0I1eWiTFzQaae-PxMre0Y/edit\n",
    "\n",
    "The task is to implement from scratch a learning algorithm for **regression** with **square loss** (e.g., **ridge regression**). The label to be predicted must be selected among the following 5 attributes, removing the remaining 4 from the dataset:\n",
    "- PERNP (Person's earnings)\n",
    "- PINCP (Person's income)\n",
    "- WAGP (Wages or salary income past 12 months)\n",
    "- HINCP (Household income)\n",
    "- FINCP (Family income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is run inside the Docker container provided in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The project is based on the analysis of the «2013 American Community Survey» dataset published on Kaggle and released under the public domain license (CC0).\n",
    "\n",
    "https://www.kaggle.com/census/2013-american-community-survey\n",
    "\n",
    "The American Community Survey is an ongoing survey from the US Census Bureau. In this survey, approximately 3.5 million households per year are asked detailed questions about who they are and how they live. Many topics are covered, including ancestry, education, work, transportation, internet use, and residency.\n",
    "\n",
    "There are two types of survey data provided, housing and population:\n",
    "- For the housing data, each row is a housing unit, and the characteristics are properties like rented vs. owned, age of home, etc.\n",
    "- For the population data, each row is a person and the characteristics are properties like age, gender, whether they work, method/length of commute, etc.\n",
    "\n",
    "Each data set is divided in two pieces, \"a\" and \"b\":\n",
    "- \"a\" contains states 1 to 25;\n",
    "- \"b\" contains states 26 to 50.\n",
    "\n",
    "Both data sets have weights associated with them. Weights are included to account for the fact that individuals are not sampled with equal probably (people who have a greater chance of being sampled have a lower weight to reflect this):\n",
    "- Weight variable for the housing data: WGTP\n",
    "- Weight variable for the population data: PWGTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/jovyan/.cache/pip/wheels/aa/e7/e7/eb3c3d514c33294d77ddd5a856bdd58dc9c1fabbed59a02a2b/kaggle-1.5.6-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.24.3)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle) (4.45.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle) (2020.4.5.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.23.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kaggle) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kaggle) (3.0.4)\n",
      "Installing collected packages: kaggle\n",
      "\u001b[33m  WARNING: The script kaggle is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed kaggle-1.5.6\n"
     ]
    }
   ],
   "source": [
    "# install kaggle API for providing the dataset\n",
    "!pip install --user kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add kaggle to PATH environment variable\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/home/jovyan/.local/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!echo '{\"username\":\"teresatanzi\",\"key\":\"a64ec0d865925975d3318adf576216b7\"}' >> ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export KAGGLE_USERNAME=teresatanzi\n",
    "!export KAGGLE_KEY=a64ec0d865925975d3318adf576216b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: kaggle 1.5.6\r\n",
      "Uninstalling kaggle-1.5.6:\r\n",
      "  Would remove:\r\n",
      "    /home/jovyan/.local/bin/kaggle\r\n",
      "    /home/jovyan/.local/lib/python3.7/site-packages/kaggle-1.5.6.dist-info/*\r\n",
      "    /home/jovyan/.local/lib/python3.7/site-packages/kaggle/*\r\n",
      "Proceed (y/n)?   Successfully uninstalled kaggle-1.5.6\r\n"
     ]
    }
   ],
   "source": [
    "# uncomment to uninstall kaggle\n",
    "#!python -c \"print('y')\" | pip uninstall kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\r\n",
      "      ____              __\r\n",
      "     / __/__  ___ _____/ /__\r\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.5\r\n",
      "      /_/\r\n",
      "                        \r\n",
      "Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_252\r\n",
      "Branch HEAD\r\n",
      "Compiled by user centos on 2020-02-02T19:38:06Z\r\n",
      "Revision cee4ecbb16917fa85f02c635925e2687400aa56b\r\n",
      "Url https://gitbox.apache.org/repos/asf/spark.git\r\n",
      "Type --help for more information.\r\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset download\n",
    "\n",
    "The dataset should not be added to the repository, but downloaded during code execution, for instance via the kaggle API \n",
    "\n",
    "https://github.com/Kaggle/kaggle-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2013-american-community-survey.zip to ./data\n",
      "100%|███████████████████████████████████████▉| 916M/916M [02:56<00:00, 5.74MB/s]\n",
      "100%|████████████████████████████████████████| 916M/916M [02:56<00:00, 5.45MB/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./data\n",
    "!kaggle datasets download census/2013-american-community-survey -p ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"./data/2013-american-community-survey.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data/2013-american-community-survey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "\n",
    "The task is to implement from scratch a learning algorithm for regression with square loss (e.g., ridge regression). The label to be predicted must be selected among the following 5 attributes, removing the remaining 4 from the dataset:\n",
    "\n",
    "- pusa dataset:\n",
    "    - PERNP (Person's earnings)\n",
    "    - PINCP (Person's income)\n",
    "    - WAGP (Wages or salary income past 12 months)\n",
    "- husa dataset:\n",
    "    - HINCP (Household income)\n",
    "    - FINCP (Family income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "baseDir = os.path.join('./data/2013-american-community-survey')\n",
    "#inputPathA = os.path.join('ss13pusa.csv')\n",
    "#inputPathB = os.path.join('ss13pusb.csv')\n",
    "inputPathA = os.path.join('ss13husa.csv')\n",
    "inputPathB = os.path.join('ss13husb.csv')\n",
    "fileNameA = os.path.join(baseDir, inputPathA)\n",
    "fileNameB = os.path.join(baseDir, inputPathB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA = sqlContext.read.csv(fileNameA, header=True)\n",
    "dfB = sqlContext.read.csv(fileNameB, header=True)\n",
    "\n",
    "df = dfB.union(dfA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n"
     ]
    }
   ],
   "source": [
    "headerList = df.columns\n",
    "print(len(headerList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RT: string (nullable = true)\n",
      " |-- SERIALNO: string (nullable = true)\n",
      " |-- DIVISION: string (nullable = true)\n",
      " |-- PUMA: string (nullable = true)\n",
      " |-- REGION: string (nullable = true)\n",
      " |-- ST: string (nullable = true)\n",
      " |-- ADJHSG: string (nullable = true)\n",
      " |-- ADJINC: string (nullable = true)\n",
      " |-- WGTP: string (nullable = true)\n",
      " |-- NP: string (nullable = true)\n",
      " |-- TYPE: string (nullable = true)\n",
      " |-- ACCESS: string (nullable = true)\n",
      " |-- ACR: string (nullable = true)\n",
      " |-- AGS: string (nullable = true)\n",
      " |-- BATH: string (nullable = true)\n",
      " |-- BDSP: string (nullable = true)\n",
      " |-- BLD: string (nullable = true)\n",
      " |-- BROADBND: string (nullable = true)\n",
      " |-- BUS: string (nullable = true)\n",
      " |-- COMPOTHX: string (nullable = true)\n",
      " |-- CONP: string (nullable = true)\n",
      " |-- DIALUP: string (nullable = true)\n",
      " |-- DSL: string (nullable = true)\n",
      " |-- ELEP: string (nullable = true)\n",
      " |-- FIBEROP: string (nullable = true)\n",
      " |-- FS: string (nullable = true)\n",
      " |-- FULP: string (nullable = true)\n",
      " |-- GASP: string (nullable = true)\n",
      " |-- HANDHELD: string (nullable = true)\n",
      " |-- HFL: string (nullable = true)\n",
      " |-- INSP: string (nullable = true)\n",
      " |-- LAPTOP: string (nullable = true)\n",
      " |-- MHP: string (nullable = true)\n",
      " |-- MODEM: string (nullable = true)\n",
      " |-- MRGI: string (nullable = true)\n",
      " |-- MRGP: string (nullable = true)\n",
      " |-- MRGT: string (nullable = true)\n",
      " |-- MRGX: string (nullable = true)\n",
      " |-- OTHSVCEX: string (nullable = true)\n",
      " |-- REFR: string (nullable = true)\n",
      " |-- RMSP: string (nullable = true)\n",
      " |-- RNTM: string (nullable = true)\n",
      " |-- RNTP: string (nullable = true)\n",
      " |-- RWAT: string (nullable = true)\n",
      " |-- RWATPR: string (nullable = true)\n",
      " |-- SATELLITE: string (nullable = true)\n",
      " |-- SINK: string (nullable = true)\n",
      " |-- SMP: string (nullable = true)\n",
      " |-- STOV: string (nullable = true)\n",
      " |-- TEL: string (nullable = true)\n",
      " |-- TEN: string (nullable = true)\n",
      " |-- TOIL: string (nullable = true)\n",
      " |-- VACS: string (nullable = true)\n",
      " |-- VALP: string (nullable = true)\n",
      " |-- VEH: string (nullable = true)\n",
      " |-- WATP: string (nullable = true)\n",
      " |-- YBL: string (nullable = true)\n",
      " |-- FES: string (nullable = true)\n",
      " |-- FFINCP: string (nullable = true)\n",
      " |-- FGRNTP: string (nullable = true)\n",
      " |-- FHINCP: string (nullable = true)\n",
      " |-- FINCP: string (nullable = true)\n",
      " |-- FPARC: string (nullable = true)\n",
      " |-- FSMOCP: string (nullable = true)\n",
      " |-- GRNTP: string (nullable = true)\n",
      " |-- GRPIP: string (nullable = true)\n",
      " |-- HHL: string (nullable = true)\n",
      " |-- HHT: string (nullable = true)\n",
      " |-- HINCP: string (nullable = true)\n",
      " |-- HUGCL: string (nullable = true)\n",
      " |-- HUPAC: string (nullable = true)\n",
      " |-- HUPAOC: string (nullable = true)\n",
      " |-- HUPARC: string (nullable = true)\n",
      " |-- KIT: string (nullable = true)\n",
      " |-- LNGI: string (nullable = true)\n",
      " |-- MULTG: string (nullable = true)\n",
      " |-- MV: string (nullable = true)\n",
      " |-- NOC: string (nullable = true)\n",
      " |-- NPF: string (nullable = true)\n",
      " |-- NPP: string (nullable = true)\n",
      " |-- NR: string (nullable = true)\n",
      " |-- NRC: string (nullable = true)\n",
      " |-- OCPIP: string (nullable = true)\n",
      " |-- PARTNER: string (nullable = true)\n",
      " |-- PLM: string (nullable = true)\n",
      " |-- PSF: string (nullable = true)\n",
      " |-- R18: string (nullable = true)\n",
      " |-- R60: string (nullable = true)\n",
      " |-- R65: string (nullable = true)\n",
      " |-- RESMODE: string (nullable = true)\n",
      " |-- SMOCP: string (nullable = true)\n",
      " |-- SMX: string (nullable = true)\n",
      " |-- SRNT: string (nullable = true)\n",
      " |-- SSMC: string (nullable = true)\n",
      " |-- SVAL: string (nullable = true)\n",
      " |-- TAXP: string (nullable = true)\n",
      " |-- WIF: string (nullable = true)\n",
      " |-- WKEXREL: string (nullable = true)\n",
      " |-- WORKSTAT: string (nullable = true)\n",
      " |-- FACCESSP: string (nullable = true)\n",
      " |-- FACRP: string (nullable = true)\n",
      " |-- FAGSP: string (nullable = true)\n",
      " |-- FBATHP: string (nullable = true)\n",
      " |-- FBDSP: string (nullable = true)\n",
      " |-- FBLDP: string (nullable = true)\n",
      " |-- FBROADBNDP: string (nullable = true)\n",
      " |-- FBUSP: string (nullable = true)\n",
      " |-- FCOMPOTHXP: string (nullable = true)\n",
      " |-- FCONP: string (nullable = true)\n",
      " |-- FDIALUPP: string (nullable = true)\n",
      " |-- FDSLP: string (nullable = true)\n",
      " |-- FELEP: string (nullable = true)\n",
      " |-- FFIBEROPP: string (nullable = true)\n",
      " |-- FFSP: string (nullable = true)\n",
      " |-- FFULP: string (nullable = true)\n",
      " |-- FGASP: string (nullable = true)\n",
      " |-- FHANDHELDP: string (nullable = true)\n",
      " |-- FHFLP: string (nullable = true)\n",
      " |-- FINSP: string (nullable = true)\n",
      " |-- FKITP: string (nullable = true)\n",
      " |-- FLAPTOPP: string (nullable = true)\n",
      " |-- FMHP: string (nullable = true)\n",
      " |-- FMODEMP: string (nullable = true)\n",
      " |-- FMRGIP: string (nullable = true)\n",
      " |-- FMRGP: string (nullable = true)\n",
      " |-- FMRGTP: string (nullable = true)\n",
      " |-- FMRGXP: string (nullable = true)\n",
      " |-- FMVP: string (nullable = true)\n",
      " |-- FOTHSVCEXP: string (nullable = true)\n",
      " |-- FPLMP: string (nullable = true)\n",
      " |-- FREFRP: string (nullable = true)\n",
      " |-- FRMSP: string (nullable = true)\n",
      " |-- FRNTMP: string (nullable = true)\n",
      " |-- FRNTP: string (nullable = true)\n",
      " |-- FRWATP: string (nullable = true)\n",
      " |-- FRWATPRP: string (nullable = true)\n",
      " |-- FSATELLITEP: string (nullable = true)\n",
      " |-- FSINKP: string (nullable = true)\n",
      " |-- FSMP: string (nullable = true)\n",
      " |-- FSMXHP: string (nullable = true)\n",
      " |-- FSMXSP: string (nullable = true)\n",
      " |-- FSTOVP: string (nullable = true)\n",
      " |-- FTAXP: string (nullable = true)\n",
      " |-- FTELP: string (nullable = true)\n",
      " |-- FTENP: string (nullable = true)\n",
      " |-- FTOILP: string (nullable = true)\n",
      " |-- FVACSP: string (nullable = true)\n",
      " |-- FVALP: string (nullable = true)\n",
      " |-- FVEHP: string (nullable = true)\n",
      " |-- FWATP: string (nullable = true)\n",
      " |-- FYBLP: string (nullable = true)\n",
      " |-- wgtp1: string (nullable = true)\n",
      " |-- wgtp2: string (nullable = true)\n",
      " |-- wgtp3: string (nullable = true)\n",
      " |-- wgtp4: string (nullable = true)\n",
      " |-- wgtp5: string (nullable = true)\n",
      " |-- wgtp6: string (nullable = true)\n",
      " |-- wgtp7: string (nullable = true)\n",
      " |-- wgtp8: string (nullable = true)\n",
      " |-- wgtp9: string (nullable = true)\n",
      " |-- wgtp10: string (nullable = true)\n",
      " |-- wgtp11: string (nullable = true)\n",
      " |-- wgtp12: string (nullable = true)\n",
      " |-- wgtp13: string (nullable = true)\n",
      " |-- wgtp14: string (nullable = true)\n",
      " |-- wgtp15: string (nullable = true)\n",
      " |-- wgtp16: string (nullable = true)\n",
      " |-- wgtp17: string (nullable = true)\n",
      " |-- wgtp18: string (nullable = true)\n",
      " |-- wgtp19: string (nullable = true)\n",
      " |-- wgtp20: string (nullable = true)\n",
      " |-- wgtp21: string (nullable = true)\n",
      " |-- wgtp22: string (nullable = true)\n",
      " |-- wgtp23: string (nullable = true)\n",
      " |-- wgtp24: string (nullable = true)\n",
      " |-- wgtp25: string (nullable = true)\n",
      " |-- wgtp26: string (nullable = true)\n",
      " |-- wgtp27: string (nullable = true)\n",
      " |-- wgtp28: string (nullable = true)\n",
      " |-- wgtp29: string (nullable = true)\n",
      " |-- wgtp30: string (nullable = true)\n",
      " |-- wgtp31: string (nullable = true)\n",
      " |-- wgtp32: string (nullable = true)\n",
      " |-- wgtp33: string (nullable = true)\n",
      " |-- wgtp34: string (nullable = true)\n",
      " |-- wgtp35: string (nullable = true)\n",
      " |-- wgtp36: string (nullable = true)\n",
      " |-- wgtp37: string (nullable = true)\n",
      " |-- wgtp38: string (nullable = true)\n",
      " |-- wgtp39: string (nullable = true)\n",
      " |-- wgtp40: string (nullable = true)\n",
      " |-- wgtp41: string (nullable = true)\n",
      " |-- wgtp42: string (nullable = true)\n",
      " |-- wgtp43: string (nullable = true)\n",
      " |-- wgtp44: string (nullable = true)\n",
      " |-- wgtp45: string (nullable = true)\n",
      " |-- wgtp46: string (nullable = true)\n",
      " |-- wgtp47: string (nullable = true)\n",
      " |-- wgtp48: string (nullable = true)\n",
      " |-- wgtp49: string (nullable = true)\n",
      " |-- wgtp50: string (nullable = true)\n",
      " |-- wgtp51: string (nullable = true)\n",
      " |-- wgtp52: string (nullable = true)\n",
      " |-- wgtp53: string (nullable = true)\n",
      " |-- wgtp54: string (nullable = true)\n",
      " |-- wgtp55: string (nullable = true)\n",
      " |-- wgtp56: string (nullable = true)\n",
      " |-- wgtp57: string (nullable = true)\n",
      " |-- wgtp58: string (nullable = true)\n",
      " |-- wgtp59: string (nullable = true)\n",
      " |-- wgtp60: string (nullable = true)\n",
      " |-- wgtp61: string (nullable = true)\n",
      " |-- wgtp62: string (nullable = true)\n",
      " |-- wgtp63: string (nullable = true)\n",
      " |-- wgtp64: string (nullable = true)\n",
      " |-- wgtp65: string (nullable = true)\n",
      " |-- wgtp66: string (nullable = true)\n",
      " |-- wgtp67: string (nullable = true)\n",
      " |-- wgtp68: string (nullable = true)\n",
      " |-- wgtp69: string (nullable = true)\n",
      " |-- wgtp70: string (nullable = true)\n",
      " |-- wgtp71: string (nullable = true)\n",
      " |-- wgtp72: string (nullable = true)\n",
      " |-- wgtp73: string (nullable = true)\n",
      " |-- wgtp74: string (nullable = true)\n",
      " |-- wgtp75: string (nullable = true)\n",
      " |-- wgtp76: string (nullable = true)\n",
      " |-- wgtp77: string (nullable = true)\n",
      " |-- wgtp78: string (nullable = true)\n",
      " |-- wgtp79: string (nullable = true)\n",
      " |-- wgtp80: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-----+------+---+-------+-------+-----+---+----+------+----+----+----+----+---+--------+----+--------+----+------+----+----+-------+---+----+----+--------+---+----+------+----+-----+----+----+----+----+--------+----+----+----+-----+----+------+---------+----+----+----+---+---+----+----+----+---+----+---+----+------+------+------+-----+-----+------+-----+-----+---+---+---------+-----+-----+------+------+---+----+-----+---+---+----+---+---+---+-----+-------+---+---+---+---+---+-------+-----+----+----+----+----+----+----+-------+--------+--------+-----+-----+------+-----+-----+----------+-----+----------+-----+--------+-----+-----+---------+----+-----+-----+----------+-----+-----+-----+--------+----+-------+------+-----+------+------+----+----------+-----+------+-----+------+-----+------+--------+-----------+------+----+------+------+------+-----+-----+-----+------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "| RT|SERIALNO|DIVISION| PUMA|REGION| ST| ADJHSG| ADJINC| WGTP| NP|TYPE|ACCESS| ACR| AGS|BATH|BDSP|BLD|BROADBND| BUS|COMPOTHX|CONP|DIALUP| DSL|ELEP|FIBEROP| FS|FULP|GASP|HANDHELD|HFL|INSP|LAPTOP| MHP|MODEM|MRGI|MRGP|MRGT|MRGX|OTHSVCEX|REFR|RMSP|RNTM| RNTP|RWAT|RWATPR|SATELLITE|SINK| SMP|STOV|TEL|TEN|TOIL|VACS|VALP|VEH|WATP|YBL| FES|FFINCP|FGRNTP|FHINCP|FINCP|FPARC|FSMOCP|GRNTP|GRPIP|HHL|HHT|    HINCP|HUGCL|HUPAC|HUPAOC|HUPARC|KIT|LNGI|MULTG| MV|NOC| NPF|NPP| NR|NRC|OCPIP|PARTNER|PLM|PSF|R18|R60|R65|RESMODE|SMOCP| SMX|SRNT|SSMC|SVAL|TAXP| WIF|WKEXREL|WORKSTAT|FACCESSP|FACRP|FAGSP|FBATHP|FBDSP|FBLDP|FBROADBNDP|FBUSP|FCOMPOTHXP|FCONP|FDIALUPP|FDSLP|FELEP|FFIBEROPP|FFSP|FFULP|FGASP|FHANDHELDP|FHFLP|FINSP|FKITP|FLAPTOPP|FMHP|FMODEMP|FMRGIP|FMRGP|FMRGTP|FMRGXP|FMVP|FOTHSVCEXP|FPLMP|FREFRP|FRMSP|FRNTMP|FRNTP|FRWATP|FRWATPRP|FSATELLITEP|FSINKP|FSMP|FSMXHP|FSMXSP|FSTOVP|FTAXP|FTELP|FTENP|FTOILP|FVACSP|FVALP|FVEHP|FWATP|FYBLP|wgtp1|wgtp2|wgtp3|wgtp4|wgtp5|wgtp6|wgtp7|wgtp8|wgtp9|wgtp10|wgtp11|wgtp12|wgtp13|wgtp14|wgtp15|wgtp16|wgtp17|wgtp18|wgtp19|wgtp20|wgtp21|wgtp22|wgtp23|wgtp24|wgtp25|wgtp26|wgtp27|wgtp28|wgtp29|wgtp30|wgtp31|wgtp32|wgtp33|wgtp34|wgtp35|wgtp36|wgtp37|wgtp38|wgtp39|wgtp40|wgtp41|wgtp42|wgtp43|wgtp44|wgtp45|wgtp46|wgtp47|wgtp48|wgtp49|wgtp50|wgtp51|wgtp52|wgtp53|wgtp54|wgtp55|wgtp56|wgtp57|wgtp58|wgtp59|wgtp60|wgtp61|wgtp62|wgtp63|wgtp64|wgtp65|wgtp66|wgtp67|wgtp68|wgtp69|wgtp70|wgtp71|wgtp72|wgtp73|wgtp74|wgtp75|wgtp76|wgtp77|wgtp78|wgtp79|wgtp80|\n",
      "+---+--------+--------+-----+------+---+-------+-------+-----+---+----+------+----+----+----+----+---+--------+----+--------+----+------+----+----+-------+---+----+----+--------+---+----+------+----+-----+----+----+----+----+--------+----+----+----+-----+----+------+---------+----+----+----+---+---+----+----+----+---+----+---+----+------+------+------+-----+-----+------+-----+-----+---+---+---------+-----+-----+------+------+---+----+-----+---+---+----+---+---+---+-----+-------+---+---+---+---+---+-------+-----+----+----+----+----+----+----+-------+--------+--------+-----+-----+------+-----+-----+----------+-----+----------+-----+--------+-----+-----+---------+----+-----+-----+----------+-----+-----+-----+--------+----+-------+------+-----+------+------+----+----------+-----+------+-----+------+-----+------+--------+-----------+------+----+------+------+------+-----+-----+-----+------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "|  H|      76|       4|01802|     2| 29|1000000|1007549|00229| 02|   1|     3|null|null|   1|  02| 07|    null|null|       2|null|  null|null| 080|   null|  2|0002| 003|       1|  3|null|     1|null| null|null|null|null|null|    null|   1|  04|   2|00720|   1|     9|     null|   1|null|   1|  1|  3|   1|null|null|  2|0360|  5|null|     0|     0|     1| null| null|  null| 0830|  027|  1|  5|000037100|    0|    4|     4|     4|  1|   1|    1|  1| 00|null|  0|  1| 00| null|      0|  1|  0|  0|  0|  0|      2| null|null|   1|   0|   0|null|null|   null|    null|       0|    0|    0|     0|    0|    0|         0|    0|         0|    0|       0|    0|    0|        0|   0|    0|    0|         0|    0|    0|    0|       0|   0|      0|     0|    0|     0|     0|   0|         0|    0|     0|    0|     0|    0|     0|       0|          0|     0|   0|     0|     0|     0|    0|    0|    0|     0|     0|    0|    0|    0|    0|00383|00063|00431|00340|00211|00212|00084|00068|00326| 00168| 00192| 00243| 00352| 00250| 00291| 00202| 00084| 00281| 00274| 00076| 00418| 00082| 00435| 00358| 00241| 00204| 00052| 00074| 00327| 00287| 00306| 00253| 00434| 00227| 00216| 00230| 00071| 00191| 00217| 00353| 00085| 00396| 00093| 00068| 00260| 00197| 00433| 00411| 00052| 00183| 00234| 00175| 00076| 00243| 00231| 00233| 00402| 00259| 00251| 00446| 00077| 00386| 00072| 00069| 00215| 00204| 00369| 00400| 00072| 00300| 00246| 00230| 00089| 00244| 00247| 00188| 00305| 00206| 00173| 00059|\n",
      "+---+--------+--------+-----+------+---+-------+-------+-----+---+----+------+----+----+----+----+---+--------+----+--------+----+------+----+----+-------+---+----+----+--------+---+----+------+----+-----+----+----+----+----+--------+----+----+----+-----+----+------+---------+----+----+----+---+---+----+----+----+---+----+---+----+------+------+------+-----+-----+------+-----+-----+---+---+---------+-----+-----+------+------+---+----+-----+---+---+----+---+---+---+-----+-------+---+---+---+---+---+-------+-----+----+----+----+----+----+----+-------+--------+--------+-----+-----+------+-----+-----+----------+-----+----------+-----+--------+-----+-----+---------+----+-----+-----+----------+-----+-----+-----+--------+----+-------+------+-----+------+------+----+----------+-----+------+-----+------+-----+------+--------+-----------+------+----+------+------+------+-----+-----+-----+------+------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We should trasform data in objects belonging to the class LabeledPoint, but first we have to solve some issues:\n",
    "\n",
    "- we have to deal with missing values: we can substitute those with mean or median of the corresponding column; <br>\n",
    "https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4\n",
    "- we have to deal with categorical values: it should be fine to just discard the feature, because only the first feature is categorical and it has a constant value for all the entry;\n",
    "- we have to deal with missing values in the labels: we could discard data points with no label, because they don't really help in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We reduced the number of attributes from 231 to 215.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "def preprocess (df, na_threshold, label, imputer_strategy):\n",
    "    \"\"\"Preprocess a PySpark DataFrame, dealing with null values.\n",
    "\n",
    "    Args:\n",
    "        df (PySpark DataFrame): DataFrame read by the csv file.\n",
    "        na_threshold (float between 0 and 1): threshold thad establish if a feature should be dropped or not\n",
    "            based on its percentage of null values.\n",
    "        label (string): feature that corresponds to the chosen label to be predicted.\n",
    "        imputer_strategy (string): it can be `mean` or `median`, based on the imputation strategy we choose.\n",
    "\n",
    "    Returns:\n",
    "        PySpark DataFrame: Restult of the processing of the original DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove RT: it is categorical and it has a constant value for all the data points in the dataset\n",
    "    df = df.drop('RT')\n",
    "    \n",
    "    # remove all the features that correspond to possible labels, but are not the chosen one\n",
    "    possible_labels = ['PERNP', 'PINCP', 'WAGP'] if inputPathA == 'ss13pusa.csv' else ['HINCP', 'FINCP']        \n",
    "    possible_labels.remove(label)\n",
    "    \n",
    "    for i in possible_labels:\n",
    "        df = df.drop(i)\n",
    "    \n",
    "    # cast features into double\n",
    "    for i in df.columns:\n",
    "        df = df.withColumn(i, df[i].cast(DoubleType()))\n",
    "        \n",
    "    # compute the dataframe containing, for each of the features, the percentange of null values\n",
    "    null_df = df.select([(count(when(col(c).isNull(), c))/n).alias(c) for c in df.columns])\n",
    "    \n",
    "    # remove form the original dataframe all the attributes that have more null values than a given threshold\n",
    "    scheme = df.columns\n",
    "    null_distr = null_df.collect()[0].asDict().values()\n",
    "    \n",
    "    for i in np.where(np.array(list(null_distr)) > na_threshold)[0]:\n",
    "        df = df.drop(scheme[i])\n",
    "        \n",
    "    print('We reduced the number of attributes from {} to {}.'.format(len(headerList), len(df.columns)))\n",
    "    \n",
    "    # remove all the data points with null value for the label\n",
    "    df = df.filter(df[label].isNotNull())\n",
    "    \n",
    "    # replace all the missing values with the mean value of the corresponding feature\n",
    "    imputer = Imputer()\n",
    "    imputer.setInputCols(df.columns)\n",
    "    imputer.setOutputCols(df.columns)\n",
    "    imputer.setStrategy(imputer_strategy)\n",
    "\n",
    "    df = imputer.fit(df).transform(df)\n",
    "\n",
    "    return df\n",
    "        \n",
    "na_threshold = .6\n",
    "label = 'HINCP'\n",
    "imputer_strategy = 'mean'\n",
    "\n",
    "df = preprocess(df, na_threshold, label, imputer_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is almost done. We may want to:\n",
    "\n",
    "- Remove column SERIALNO (it is just a unique number for every observation);\n",
    "- Tune the threshold for the feature dropping based on the percentage of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled points\n",
    "\n",
    "Let us now cast all the data point into `LabeledPoint`. The label is the one we select from the 5 different possible labels given by the project, while all the other possible labels have to be removed from the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76.0,4.0,1802.0,2.0,29.0,1000000.0,1007549.0,229.0,2.0,1.0,3.0,1.3062936394368012,1.0,2.0,7.0,1.5566063335988354,1.9844443820931135,2.0,1.9596121838007086,1.6977955999890137,80.0,1.890910489164767,2.0,2.0,3.0,1.0,3.0,910.6082502817558,1.0,1.44533933917438,1.7879980881910393,1.9832184350023345,1.0,4.0,1.0,9.0,1.932411217006784,1.0,1.0,1.0,3.0,1.0,248086.86971966035,2.0,360.0,5.0,3.123169246051848,0.0,1.0,3.134906236307153,0.31369008632643547,1.0,5.0,0.0,4.0,4.0,4.0,1.0,1.0,1.0,1.0,0.0,3.0029736555778195,0.0,1.0,0.0,24.084478191098956,0.0,1.0,0.0,0.0,0.0,0.0,2.0,1227.739407457235,1.0,0.0,0.0,32.50750385017082,1.4557422569258112,6.382029815328111,5.843664053098351,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,383.0,63.0,431.0,340.0,211.0,212.0,84.0,68.0,326.0,168.0,192.0,243.0,352.0,250.0,291.0,202.0,84.0,281.0,274.0,76.0,418.0,82.0,435.0,358.0,241.0,204.0,52.0,74.0,327.0,287.0,306.0,253.0,434.0,227.0,216.0,230.0,71.0,191.0,217.0,353.0,85.0,396.0,93.0,68.0,260.0,197.0,433.0,411.0,52.0,183.0,234.0,175.0,76.0,243.0,231.0,233.0,402.0,259.0,251.0,446.0,77.0,386.0,72.0,69.0,215.0,204.0,369.0,400.0,72.0,300.0,246.0,230.0,89.0,244.0,247.0,188.0,305.0,206.0,173.0,59.0], 37100.0\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parsePoint(row):\n",
    "    \"\"\"Converts a row of a pyspark dataframe into a `LabeledPoint`.\n",
    "\n",
    "    Args:\n",
    "        row: Row of a dataframe composed all of double values. One of those values corresponds to the label, while\n",
    "            other possible lables have to be removed.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The line is converted into a `LabeledPoint`, which consists of a label and\n",
    "            features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # cast row into a dictionary\n",
    "    row_dict = row.asDict()\n",
    "    label_value = row_dict[label]\n",
    "    \n",
    "    del row_dict[label]\n",
    "    feature_list = list(row_dict.values())\n",
    "    \n",
    "    return LabeledPoint(label_value, feature_list)\n",
    "\n",
    "#parsedSamplePoints = list(map(parsePoint, df.take(5)))\n",
    "#firstPointFeatures = parsedSamplePoints[0].features\n",
    "#firstPointLabel = parsedSamplePoints[0].label\n",
    "#print('{}, {}'.format(firstPointFeatures, firstPointLabel))\n",
    "\n",
    "parsedData = df.rdd.map(lambda s: parsePoint(s))\n",
    "\n",
    "parsedExamplePoint = parsedData.take(1)\n",
    "examplePointFeatures = parsedExamplePoint[0].features\n",
    "examplePointLabel = parsedExamplePoint[0].label\n",
    "print('{}, {}'.format(examplePointFeatures, examplePointLabel))\n",
    "\n",
    "d = len(examplePointFeatures)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsedData.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning algorithm\n",
    "\n",
    "### Training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[77] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lentissimo (può darsi che sia solo per i count)\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData, parsedTestData = parsedData.randomSplit(weights, seed = seed)\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "parsedTestData.cache()\n",
    "#nTrain = parsedTrainData.count()\n",
    "#nVal = parsedValData.count()\n",
    "#nTest = parsedTestData.count()\n",
    "\n",
    "#print('{} {} {} {}'.format(nTrain, nVal, nTest, nTrain + nVal + nTest))\n",
    "#print(parsedData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(37100.0, [76.0,4.0,1802.0,2.0,29.0,1000000.0,1007549.0,229.0,2.0,1.0,3.0,1.3062936394368012,1.0,2.0,7.0,1.5566063335988354,1.9844443820931135,2.0,1.9596121838007086,1.6977955999890137,80.0,1.890910489164767,2.0,2.0,3.0,1.0,3.0,910.6082502817558,1.0,1.44533933917438,1.7879980881910393,1.9832184350023345,1.0,4.0,1.0,9.0,1.932411217006784,1.0,1.0,1.0,3.0,1.0,248086.86971966035,2.0,360.0,5.0,3.123169246051848,0.0,1.0,3.134906236307153,0.31369008632643547,1.0,5.0,0.0,4.0,4.0,4.0,1.0,1.0,1.0,1.0,0.0,3.0029736555778195,0.0,1.0,0.0,24.084478191098956,0.0,1.0,0.0,0.0,0.0,0.0,2.0,1227.739407457235,1.0,0.0,0.0,32.50750385017082,1.4557422569258112,6.382029815328111,5.843664053098351,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,383.0,63.0,431.0,340.0,211.0,212.0,84.0,68.0,326.0,168.0,192.0,243.0,352.0,250.0,291.0,202.0,84.0,281.0,274.0,76.0,418.0,82.0,435.0,358.0,241.0,204.0,52.0,74.0,327.0,287.0,306.0,253.0,434.0,227.0,216.0,230.0,71.0,191.0,217.0,353.0,85.0,396.0,93.0,68.0,260.0,197.0,433.0,411.0,52.0,183.0,234.0,175.0,76.0,243.0,231.0,233.0,402.0,259.0,251.0,446.0,77.0,386.0,72.0,69.0,215.0,204.0,369.0,400.0,72.0,300.0,246.0,230.0,89.0,244.0,247.0,188.0,305.0,206.0,173.0,59.0])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedTrainData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(94050.0, [695.0,4.0,2001.0,2.0,29.0,1000000.0,1007549.0,74.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,2.0,2.0,1.0,80.0,2.0,2.0,2.0,3.0,1.0,3.0,1600.0,1.0,2.0,1.0,2.0,1.0,5.0,1.0,9.0,2.0,1.0,1.0,1.0,1.0,1.0,95000.0,2.0,60.0,6.0,1.0,0.0,0.0,4.0,0.0,1.0,1.0,0.0,4.0,4.0,4.0,1.0,1.0,1.0,6.0,0.0,2.0,0.0,0.0,0.0,10.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,808.0,0.0,0.0,1.0,19.0,2.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,116.0,20.0,22.0,61.0,74.0,95.0,126.0,95.0,23.0,67.0,22.0,23.0,123.0,65.0,102.0,68.0,69.0,159.0,85.0,82.0,27.0,123.0,143.0,73.0,66.0,69.0,21.0,70.0,116.0,76.0,117.0,137.0,25.0,70.0,29.0,79.0,69.0,21.0,76.0,61.0,26.0,134.0,135.0,82.0,62.0,61.0,22.0,71.0,120.0,75.0,121.0,110.0,26.0,86.0,25.0,101.0,71.0,20.0,71.0,74.0,122.0,23.0,23.0,74.0,80.0,71.0,131.0,72.0,23.0,82.0,19.0,23.0,152.0,83.0,120.0,78.0,74.0,125.0,78.0,80.0])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedValData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(41000.0, [802.0,4.0,1004.0,2.0,29.0,1000000.0,1007549.0,73.0,3.0,1.0,1.0,1.0,1.0,4.0,2.0,2.0,2.0,2.0,2.0,1.0,360.0,2.0,2.0,2.0,50.0,2.0,3.0,1300.0,1.0,2.0,1.0,2.0,1.0,10.0,1.0,9.0,2.0,1.0,1.0,1.0,1.0,1.0,300000.0,2.0,50.0,13.0,2.0,1.0,1.0,4.0,0.0,1.0,1.0,0.0,4.0,4.0,4.0,1.0,1.0,1.0,2.0,0.0,3.0,0.0,0.0,0.0,68.0,0.0,1.0,0.0,0.0,1.0,0.0,3.0,2314.0,0.0,0.0,1.0,68.0,2.0,7.0,3.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,104.0,23.0,67.0,147.0,69.0,62.0,62.0,114.0,66.0,104.0,14.0,83.0,22.0,88.0,69.0,63.0,30.0,19.0,122.0,68.0,134.0,71.0,25.0,80.0,20.0,23.0,124.0,78.0,89.0,67.0,86.0,97.0,69.0,108.0,21.0,21.0,76.0,93.0,64.0,14.0,60.0,123.0,84.0,18.0,92.0,90.0,67.0,17.0,74.0,32.0,126.0,83.0,93.0,77.0,88.0,78.0,130.0,119.0,24.0,76.0,20.0,65.0,136.0,62.0,115.0,121.0,26.0,63.0,22.0,94.0,95.0,19.0,59.0,22.0,108.0,122.0,68.0,65.0,77.0,170.0])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedTestData.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should shift labels in order to make them begin from 0 (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2090000.0 -19770.0\n"
     ]
    }
   ],
   "source": [
    "# lento\n",
    "\n",
    "onlyLabels = parsedData.map(lambda p: p.label)\n",
    "minLbl = onlyLabels.min()\n",
    "maxLbl = onlyLabels.max()\n",
    "print(maxLbl, minLbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average baseline model\n",
    "\n",
    "We predict for every data point the avarage label of the points in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75229.84229664435\n"
     ]
    }
   ],
   "source": [
    "averageTrainLbl = (parsedTrainData\n",
    "                    .map(lambda p: p.label)\n",
    "                    .mean())\n",
    "\n",
    "print(averageTrainLbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Square Loss\n",
    "\n",
    "\\begin{equation}\n",
    "    l(\\underline{w}) = \\sum_{j=1}^n (\\hat{y}^{(j)} - y^{(j)})^2\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\underline{w}$ is the weight vector;\n",
    "- $\\hat{y}^{(j)} = \\underline{w} \\cdot \\underline{x}^{(j)}$ is our prediction with respect to the $j$-th observation;\n",
    "- $y^{(j)}$ is the actual label of the $j$-th observation.\n",
    "\n",
    "\\begin{equation}\n",
    "    l(\\underline{w}) = \\sum_{j=1}^n (\\underline{w} \\cdot \\underline{x}^{(j)} - y^{(j)})^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squaredError(label, prediction):\n",
    "    \"\"\"Calculates the the squared error for a single prediction.\n",
    "\n",
    "    Args:\n",
    "        label (float): The correct value for this observation.\n",
    "        prediction (float): The predicted value for this observation.\n",
    "\n",
    "    Returns:\n",
    "        float: The difference between the `label` and `prediction` squared.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (label - prediction) ** 2\n",
    "\n",
    "def calcRMSE(labelsAndPreds):\n",
    "    \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n",
    "\n",
    "    Args:\n",
    "        labelsAndPred (RDD of (float, float)): An `RDD` consisting of (label, prediction) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The square root of the mean of the squared errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sqrt(labelsAndPreds.map(lambda p: squaredError(*p)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2909944487358056\n"
     ]
    }
   ],
   "source": [
    "labelsAndPreds = sc.parallelize([(3., 1.), (1., 2.), (2., 2.)])\n",
    "# RMSE = sqrt[((3-1)^2 + (1-2)^2 + (2-2)^2) / 3] = 1.291\n",
    "exampleRMSE = calcRMSE(labelsAndPreds)\n",
    "print(exampleRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 65, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1065, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-14-4267e37cc023>\", line 1, in <lambda>\nNameError: name 'averageTrainLbl' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1065, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-14-4267e37cc023>\", line 1, in <lambda>\nNameError: name 'averageTrainLbl' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4267e37cc023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabelsAndPredsTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsedTrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverageTrainLbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrmseTrainBase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelsAndPredsTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabelsAndPredsVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsedValData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverageTrainLbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrmseValBase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelsAndPredsVal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c7718639a6dd>\u001b[0m in \u001b[0;36mcalcRMSE\u001b[0;34m(labelsAndPreds)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \"\"\"\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelsAndPreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \"\"\"\n\u001b[0;32m-> 1202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 65, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1065, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-14-4267e37cc023>\", line 1, in <lambda>\nNameError: name 'averageTrainLbl' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1065, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/statcounter.py\", line 42, in __init__\n    for v in values:\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-14-4267e37cc023>\", line 1, in <lambda>\nNameError: name 'averageTrainLbl' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "labelsAndPredsTrain = parsedTrainData.map(lambda p: (p.label, averageTrainLbl))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "labelsAndPredsVal = parsedValData.map(lambda p: (p.label, averageTrainLbl))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "\n",
    "labelsAndPredsTest = parsedTestData.map(lambda p: (p.label, averageTrainLbl))\n",
    "rmseTestBase = calcRMSE(labelsAndPredsTest)\n",
    "\n",
    "print('Baseline Train RMSE = {0:.3f}'.format(rmseTrainBase))\n",
    "print('Baseline Validation RMSE = {0:.3f}'.format(rmseValBase))\n",
    "print('Baseline Test RMSE = {0:.3f}'.format(rmseTestBase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "Learn the best weight vector $\\underline{w}^{*}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\underline{w}^{*} = \\operatorname*{argmin}_{\\underline{w}} ||X \\cdot \\underline{w} - \\underline{y}||^2 + \\lambda ||\\underline{w}||^2\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "- $X$ is a matrix where each row is a data point;\n",
    "- $\\underline{y}$ is the vector of the labels;\n",
    "- $\\lambda > 0$ is the regularization parameter (it has to be chosen before hands).\n",
    "\n",
    "We can find the optimal value $\\underline{w}^{*}$ computing the gradient of the function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\underline{w}^{*} = (X^T X + \\lambda I)^{-1} X^T \\underline{y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Gradient descent\n",
    "\n",
    "If the number $d$ of features and the number $n$ of data points in our dataset are too big, the complexity in time and space of our algorithm may be too high and we may not be able to compute the optimal value for $\\underline{w}$. To solve this problem, we can rely on the gradient descent procedure:\n",
    "\n",
    "1. choose $\\underline{w}_0 \\in R^d$\n",
    "2. $i = 0$\n",
    "3. while (!stop) {\n",
    "    1. $\\underline{w}_{i+1} = \\underline{w}_i - \\xi_i \\left[ \\sum_{j=1}^n (\\underline{w}_i \\cdot \\underline{x}^{(j)} - y^{(j)}) \\underline{x}^{(j)} + \\lambda \\underline{w}_i \\right]$\n",
    "    2. i++\n",
    "4. }\n",
    "5. return $\\underline{w}_i$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\underline{w_i}$ is the approximated optimal weight vector;\n",
    "- stop is a termination criterion of our choice;\n",
    "- $\\xi_i$ is the learning rate:\n",
    "    \\begin{equation}\n",
    "        \\xi_i = \\frac{\\xi_0}{n \\sqrt{i}}\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us focus on the update:\n",
    "\\begin{equation}\n",
    "    \\underline{w}_{i+1} = \\underline{w}_i - \\xi_i \\left[ \\sum_{j=1}^n (\\underline{w}_i \\cdot \\underline{x}^{(j)} - y^{(j)}) \\underline{x}^{(j)} + \\lambda \\underline{w}_i \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Let us begin writing a function that computes the summand of the update: $(\\underline{w}_i \\cdot \\underline{x}^{(j)} - y^{(j)}) \\underline{x}^{(j)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "\n",
    "def gradientSummand(weights, lp):\n",
    "    \"\"\"Calculates the gradient summand for a given weight and `LabeledPoint`.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        weights (DenseVector): An array of model weights (betas).\n",
    "        lp (LabeledPoint): The `LabeledPoint` for a single observation.\n",
    "\n",
    "    Returns:\n",
    "        DenseVector: An array of values the same length as `weights`.  The gradient summand.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (weights.dot(DenseVector(lp.features)) - lp.label) * lp.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.0,6.0,24.0]\n",
      "[1.7304000000000002,-5.191200000000001,-2.5956000000000006]\n"
     ]
    }
   ],
   "source": [
    "exampleW = DenseVector([1, 1, 1])\n",
    "exampleLP = LabeledPoint(2.0, [3, 1, 4])\n",
    "# gradientSummand = (dot([1 1 1], [3 1 4]) - 2) * [3 1 4] = (8 - 2) * [3 1 4] = [18 6 24]\n",
    "summandOne = gradientSummand(exampleW, exampleLP)\n",
    "print(summandOne)\n",
    "\n",
    "exampleW = DenseVector([.24, 1.2, -1.4])\n",
    "exampleLP = LabeledPoint(3.0, [-1.4, 4.2, 2.1])\n",
    "summandTwo = gradientSummand(exampleW, exampleLP)\n",
    "print(summandTwo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write the function that makes the prediction, based on the weights vector $\\underline{w}$ and the observation $\\underline{x}^{(j)}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y}^{(j)} = \\underline{w} \\cdot \\underline{x}^{(j)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabeledPrediction(weights, observation):\n",
    "    \"\"\"Calculates predictions and returns a (label, prediction) tuple.\n",
    "\n",
    "    Note:\n",
    "        The labels should remain unchanged as we'll use this information to calculate prediction\n",
    "        error later.\n",
    "\n",
    "    Args:\n",
    "        weights (np.ndarray): An array with one weight for each features in `trainData`.\n",
    "        observation (LabeledPoint): A `LabeledPoint` that contain the correct label and the\n",
    "            features for the data point.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A (label, prediction) tuple.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (observation.label, weights.dot(DenseVector(observation.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.0, 1.75), (1.5, 1.25)]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([1.0, 1.5])\n",
    "predictionExample = sc.parallelize([LabeledPoint(2, np.array([1.0, .5])),\n",
    "                                    LabeledPoint(1.5, np.array([.5, .5]))])\n",
    "labelsAndPredsExample = predictionExample.map(lambda lp: getLabeledPrediction(weights, lp))\n",
    "print(labelsAndPredsExample.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can write the code that learns the best weight vector $\\underline{w}^*$ using the gradient descent procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridgeRegression(trainData, nIters, regFactor):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with gradient descent.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        trainData (RDD of LabeledPoint): The labeled data for use in training the model.\n",
    "        nIters (int): The number of iterations of gradient descent to perform.\n",
    "        regFactor (float greater then 0): Regularization factor used in ridge regression.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # number of data points in training set\n",
    "    # TODO: count è lenta\n",
    "    n = trainData.count()\n",
    "    \n",
    "    # number of dimensions\n",
    "    d = len(trainData.take(1)[0].features)\n",
    "    \n",
    "    # initialize weight vector with a vector of d zero components\n",
    "    w = np.zeros(d)\n",
    "    \n",
    "    # initialize learning rate\n",
    "    csi = 1\n",
    "    \n",
    "    # we will compute e store the training error after each iteration in order to evaluate the process\n",
    "    errorTrain = np.zeros(nIters)\n",
    "    \n",
    "    for i in range(nIters):\n",
    "        # make the prediction with the current learnt weight vector\n",
    "        labelsAndPredsTrain = trainData.map(lambda p: getLabeledPrediction(w, p))\n",
    "        \n",
    "        # compute the error and add it to the list\n",
    "        errorTrain[i] = calcRMSE(labelsAndPredsTrain)\n",
    "        \n",
    "        # compute the gradient\n",
    "        # TODO: collect è lenta\n",
    "        gradient = sum([DenseVector(gradientSummand(w, lp)) for lp in trainData.collect()])\n",
    "        #+ (regFactor * w)\n",
    "        \n",
    "        # update the learning rate\n",
    "        csi = csi / (n * np.sqrt(i + 1))\n",
    "        \n",
    "        # update the weights\n",
    "        w -= csi * gradient\n",
    "        \n",
    "    return w, errorTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(37100.0, [76.0,4.0,1802.0]), LabeledPoint(76000.0, [86.0,4.0,1600.0])]\n",
      "[1.30412730e+21 1.36480985e+19 6.28608319e+21]\n"
     ]
    }
   ],
   "source": [
    "# create a toy dataset with n = 10, d = 3, and then run 5 iterations of gradient descent\n",
    "# note: the resulting model will not be useful; the goal here is to verify that\n",
    "# linregGradientDescent is working properly\n",
    "exampleN = 10\n",
    "exampleD = 3\n",
    "exampleData = (sc\n",
    "               .parallelize(parsedTrainData.take(exampleN))\n",
    "               .map(lambda lp: LabeledPoint(lp.label, lp.features[0:exampleD])))\n",
    "print(exampleData.take(2))\n",
    "exampleNumIters = 5\n",
    "exampleWeights, exampleErrorTrain = ridgeRegression(exampleData, exampleNumIters, 0)\n",
    "print(exampleWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train our ridge regressor on the trainig data we have and evaluate it on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numIters = 500\n",
    "regFactor = .5\n",
    "\n",
    "weightsLR0, errorTrainLR0 = ridgeRegression(parsedTrainData, numIters, regFactor)\n",
    "\n",
    "labelsAndPreds = parsedValData.map(lambda p: getLabeledPrediction(weightsLR0, p))\n",
    "rmseValLR0 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}'.format(rmseValBase,\n",
    "                                                                       rmseValLR0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
